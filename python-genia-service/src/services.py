"""
Servicios de negocio para integraci√≥n con Google Gemini API.
Implementaci√≥n siguiendo la documentaci√≥n oficial de Google.
"""
import time
import asyncio
from typing import Optional, Dict, Any
from config import settings
from models import QueryRequest, QueryResponse
from logging_config import get_logger, log_api_call, log_performance
from google import genai

# Obtener logger espec√≠fico para este m√≥dulo
logger = get_logger(__name__)

class GeniaAPIService:
    """
    Servicio para interactuar con Google Gemini API
    Implementaci√≥n oficial siguiendo: https://ai.google.dev/gemini-api/docs/quickstart
    """
    
    def __init__(self):
        self.api_key = settings.GENIA_API_KEY
        self.timeout = settings.API_TIMEOUT
        self.max_retries = settings.MAX_RETRIES
        self.model_name = "gemini-1.5-flash"
        
        logger.info(f"üîß Initializing GeniaAPIService with model: {self.model_name}")
        logger.debug(f"Service configuration: timeout={self.timeout}s, max_retries={self.max_retries}")
        
        # Configurar el cliente oficial de Google Gemini
        if self.api_key:
            try:
                logger.debug("Setting up Google Gemini client...")
                self.client = genai.Client(api_key=self.api_key)
                logger.info(f"‚úÖ Google Gemini client configured successfully")
                logger.debug(f"API Key preview: {self.api_key[:10]}...{self.api_key[-5:]}")
            except Exception as e:
                logger.error(f"‚ùå Failed to configure Google Gemini client: {e}")
                logger.debug(f"Error type: {type(e).__name__}")
                self.client = None
        else:
            logger.warning("‚ö†Ô∏è  No GENIA_API_KEY provided. Service will work in mock mode only.")
            self.client = None
    
    async def query(self, request: QueryRequest) -> QueryResponse:
        """
        Realizar consulta a Google Gemini API usando el SDK oficial
        
        Args:
            request: Datos de la consulta con prompt, max_tokens, temperature
            
        Returns:
            QueryResponse: Respuesta de Google Gemini
            
        Raises:
            Exception: Error en la comunicaci√≥n con Google Gemini
        """
        if not self.client:
            logger.error("‚ùå Google Gemini client not configured")
            raise Exception("Google Gemini client not configured. Check your GENIA_API_KEY.")
        
        start_time = time.time()
        call_id = f"gemini_{int(start_time * 1000)}"
        
        try:
            logger.info(f"üöÄ [{call_id}] Calling Google Gemini API ({self.model_name})")
            logger.debug(f"[{call_id}] Prompt preview: '{request.prompt[:100]}...'")
            logger.debug(f"[{call_id}] Prompt length: {len(request.prompt)} characters")
            
            # Preparar par√°metros seg√∫n la documentaci√≥n oficial
            generation_config = self._build_generation_config(request)
            logger.debug(f"[{call_id}] Generation config: {generation_config}")
            
            # Ejecutar la llamada en un thread pool para hacerla async
            logger.debug(f"[{call_id}] Executing API call...")
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                self._generate_content_with_config, 
                request.prompt,
                generation_config
            )
            
            processing_time = time.time() - start_time
            estimated_tokens = self._estimate_tokens(response.text)
            
            # Log m√©tricas de performance
            log_performance(
                f"gemini_api_call_{call_id}",
                processing_time,
                {
                    "model": self.model_name,
                    "prompt_length": len(request.prompt),
                    "response_length": len(response.text),
                    "estimated_tokens": estimated_tokens
                }
            )
            
            logger.info(f"‚úÖ [{call_id}] Google Gemini API success - Time: {processing_time:.3f}s, Tokens: {estimated_tokens}")
            
            return QueryResponse(
                response=response.text,
                tokens_used=estimated_tokens,
                model=self.model_name,
                processing_time=processing_time
            )
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_details = {
                "model": self.model_name,
                "prompt_length": len(request.prompt),
                "error_type": type(e).__name__,
                "processing_time": processing_time
            }
            
            logger.error(f"‚ùå [{call_id}] Google Gemini API failed: {str(e)} - Time: {processing_time:.3f}s")
            logger.debug(f"[{call_id}] Error details: {error_details}")
            
            # Log la llamada fallida para monitoring
            log_api_call("POST", f"google-gemini/{self.model_name}", 500, processing_time)
            
            raise Exception(f"Google Gemini API error: {str(e)}. Details: {error_details}")
    
    def _build_generation_config(self, request: QueryRequest) -> Optional[Dict[str, Any]]:
        """
        Construir configuraci√≥n de generaci√≥n basada en los par√°metros del request
        Soporta todos los par√°metros de gemini-1.5-pro-002
        """
        config = {}
        
        # Mapear par√°metros de nuestro modelo a los de Gemini
        if request.max_tokens is not None:
            config['max_output_tokens'] = request.max_tokens
        
        if request.temperature is not None:
            config['temperature'] = request.temperature
        
        if hasattr(request, 'top_p') and request.top_p is not None:
            config['top_p'] = request.top_p
        
        if hasattr(request, 'top_k') and request.top_k is not None:
            config['top_k'] = request.top_k
        
        # Par√°metros por defecto optimizados para gemini-1.5-pro-002
        config.setdefault('max_output_tokens', 2048)
        config.setdefault('temperature', 0.7)
        
        return config if config else None
    
    def _generate_content_with_config(self, prompt: str, generation_config: Optional[Dict[str, Any]] = None):
        """
        M√©todo sincr√≥nico para generar contenido con configuraci√≥n
        Siguiendo exactamente la documentaci√≥n oficial de Google
        """
        # Implementaci√≥n simplificada - exactamente como en la documentaci√≥n oficial
        return self.client.models.generate_content(
            model=self.model_name,
            contents=prompt
        )
    
    def _estimate_tokens(self, text: str) -> int:
        """
        Estimaci√≥n de tokens m√°s precisa para gemini-1.5-pro-002
        """
        # Gemini 1.5 Pro tiene tokenizaci√≥n m√°s eficiente
        # Estimaci√≥n mejorada: ~3.5 caracteres por token en promedio para espa√±ol
        return max(1, len(text) // 4)
    
    async def query_mock(self, request: QueryRequest) -> QueryResponse:
        """
        Consulta mock para testing sin llamar a Google Gemini real
        
        Args:
            request: Datos de la consulta
            
        Returns:
            QueryResponse: Respuesta simulada
        """
        start_time = time.time()
        
        # Simular latencia de API real de Gemini
        await asyncio.sleep(0.3)  # Gemini 1.5 Pro es m√°s r√°pido
        
        processing_time = time.time() - start_time
        
        logger.info(f"Mock query processed in {processing_time:.2f}s")
        
        # Respuesta mock m√°s realista
        mock_response = (
            f"[MOCK GEMINI-1.5-PRO-002] Procesando consulta: '{request.prompt}'\n\n"
            f"Esta ser√≠a una respuesta generada por Google Gemini 1.5 Pro 002 "
            f"con temperatura={getattr(request, 'temperature', 0.7)} y "
            f"max_tokens={getattr(request, 'max_tokens', 2048)}.\n\n"
            f"El modelo gemini-1.5-pro-002 es conocido por su alta calidad en "
            f"generaci√≥n de texto, razonamiento complejo y comprensi√≥n contextual avanzada."
        )
        
        return QueryResponse(
            response=mock_response,
            tokens_used=len(request.prompt.split()) * 2,  # Mock m√°s realista
            model=f"mock-{self.model_name}",
            processing_time=processing_time
        )
    
    async def health_check(self) -> bool:
        """
        Verificar salud de Google Gemini API con una consulta simple
        
        Returns:
            bool: True si la API est√° disponible y funcionando
        """
        if not self.client:
            logger.warning("Health check failed: Client not configured")
            return False
            
        try:
            logger.info(f"Performing health check with {self.model_name}")
            
            # Consulta simple y r√°pida para verificar conectividad
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self._generate_content_with_config,
                "Hello, respond with just 'OK'",
                {"max_output_tokens": 10, "temperature": 0.1}
            )
            
            # Verificar que la respuesta sea v√°lida
            is_healthy = bool(response and response.text and len(response.text.strip()) > 0)
            
            if is_healthy:
                logger.info(f"Health check passed: {self.model_name} is healthy")
            else:
                logger.warning(f"Health check failed: Invalid response from {self.model_name}")
                
            return is_healthy
                
        except Exception as e:
            logger.warning(f"Google Gemini API health check failed: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Obtener informaci√≥n del modelo configurado
        """
        return {
            "model_name": self.model_name,
            "client_configured": bool(self.client),
            "api_key_set": bool(self.api_key),
            "capabilities": {
                "text_generation": True,
                "multimodal": True,  # gemini-1.5-pro-002 soporta im√°genes
                "long_context": True,  # Hasta 1M tokens de contexto
                "function_calling": True,
                "json_mode": True
            },
            "limits": {
                "max_output_tokens": 8192,
                "context_window": 1048576,  # 1M tokens
                "temperature_range": [0.0, 2.0]
            }
        }

# Instancia global del servicio
genia_service = GeniaAPIService()